{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can Whisper Language Model Like GPT?\n",
    "In this tutorial inspired by Ellena Reid's work (https://www.lesswrong.com/posts/thePw6qdyabD8XR4y/interpreting-openai-s-whisper#1_2__Residual_Stream_Features) we explore whether or not we can use Whisper as a Language Model (LM).\n",
    "\n",
    "She establishes that the most likely word following some initial word, is often quite sensible (among other things). However, she doesn't give us examples of entire transcriptions at different temperature controls, so it's hard to tell if Whisper can actually be used to generate seemingly sensible completions. To explore that, we use the TinyStories dataset: https://huggingface.co/datasets/roneneldan/TinyStories. We condition the language model using prefixes of different lengths and and just let it run. We explore a couple different options for the background including (1) zero (i.e. pure silence), (2) average of many random transcripts, filtered and attenuated to sound like background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
